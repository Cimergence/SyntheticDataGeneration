{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Synthetic Dataset\n",
    "In this notebook we will use NIM to generate synthetic data for an LLM  \n",
    "\n",
    "The topic will be about **occupational health and safety**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmZsb3djaGFydCBUQgogICAgQVtNZXRhLUxsYW1hLTMuMS00MDVCXSAtLT5CW0luc3RydWN0aW9uc10KICAgIHN1YmdyYXBoIFN1YnRvcGljcwogICAgQiAtLT4gQ1tHaXQgQ29tbWFuZF0KICAgIEMgLS0+IERbRmlsdGVyIFJlc3BvbnNlXQogICAgZW5kCiAgICBzdWJncmFwaCBSZXdhcmQgTW9kZWwKICAgIEQgLS0+RVtGaWx0ZXIgUmVzcG9uc2UgLyBOZW1vdHJvbi00LTM0MEItUmV3YXJkXQogICAgZW5kCiAgICBFIC0tPiBGW0RhdGFzZXRdCg==\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import base64\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mm(graph):\n",
    "    graphbytes = graph.encode(\"utf8\")\n",
    "    base64_bytes = base64.b64encode(graphbytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    display(Image(url=\"https://mermaid.ink/img/\" + base64_string))\n",
    "\n",
    "mm(\"\"\"\n",
    "flowchart TB\n",
    "    A[Meta-Llama-3.1-405B] -->B[Instructions]\n",
    "    subgraph Subtopics\n",
    "    B --> C[Git Command]\n",
    "    C --> D[Filter Response]\n",
    "    end\n",
    "    subgraph Reward Model\n",
    "    D -->E[Filter Response / Nemotron-4-340B-Reward]\n",
    "    end\n",
    "    E --> F[Dataset]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "Python libraries\n",
    "- openai (we will use it only for test >> we will have our own caller function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rich import print\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Client\n",
    "We define the different information (api-key, model...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "  api_key=os.environ[\"NVIDIA_API_KEY\"]\n",
    ")\n",
    "\n",
    "MODEL = \"meta/llama-3.1-405b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"meta/llama-3.1-405b-instruct\",\n",
    "  messages=[{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}],\n",
    "  temperature=0.2,\n",
    "  top_p=0.7,\n",
    "  max_tokens=1024,\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "  if chunk.choices[0].delta.content is not None:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
